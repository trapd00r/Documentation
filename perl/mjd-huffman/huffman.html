<HTML>
<HEAD>
<TITLE>Bricolage: Data Compression</TITLE>
<LINK REV="made" HREF="mailto:mjd-perl-huffman@plover.com">
</HEAD>

<BODY BGCOLOR="white">

<h1><em>Bricolage:</em> Data Compression</h1>

<p align=right><font size=-1>&copy; Copyright 1998 The Perl Journal.  Reprinted with permission.</font></p>


<!-- INDEX BEGIN -->

<UL>

	<LI><A HREF="#Bricolage_Data_Compression">Bricolage: Data Compression</A>
	<UL>

		<LI><A HREF="#Morse_Code">Morse Code</A>
		<LI><A HREF="#Ambiguous_codes">Ambiguous codes</A>
		<LI><A HREF="#Huffman_Coding">Huffman Coding</A>
		<LI><A HREF="#The_Code">The Code</A>
		<LI><A HREF="#The_Rub">The Rub</A>
		<LI><A HREF="#Another_Rub">Another Rub</A>
		<LI><A HREF="#Other_Methods">Other Methods</A>
		<LI><A HREF="#Other_Directions">Other Directions</A>
		<LI><A HREF="#Bibliography">Bibliography</A>
		<LI><A HREF="#Notes">Notes</A>
	</UL>

</UL>
<!-- INDEX END -->

<HR>
<P>
<H1><A NAME="Bricolage_Data_Compression">Bricolage: Data Compression</A></H1>
<P>
<img src="../pics/medium-sigils.gif" align=left>
You are probably familiar with Unix <CODE>compress</CODE>, <CODE>gzip</CODE>, or <CODE>bzip2</CODE>
utilities, or the DOS <CODE>pkzip</CODE> utility. These programs all make files smaller; we say that such files are <EM>compressed</EM>. Compressed files take less disk space and less network bandwidth. The
downside of compressed files is that it they are full of unreadable
gibberish; you usually have to run another program to <EM>uncompress</EM> them before you can use them again. In this article we'll see how file
compression works, and I'll show a simple module that includes functions
for compressing and uncompressing files.

<P>
<HR>
<H2><A NAME="Morse_Code">Morse Code</A></H2>
<P>
The idea behind data compression is very simple. In a typical file, say a
text file, every character takes up the same amount of space: 8 bits. The
letter <CODE>e</CODE> is represented by the 8 bits 01100101; the letter Z is represented by the 8
bits 010101010. But in a text file,
<CODE>e</CODE> occurs much more frequently than <CODE>Z</CODE>---maybe about 75 times as frequently. If you could give the common symbols
short codes and the uncommon symbols long codes, you'd have a net gain.

<P>
This isn't a new idea. It was exploited by Samuel Morse in the Morse Code,
a very early digital data transmission protocol. Morse Code was designed to
send text files over telegraph wires. A telegraph is very simple; it has a
switch at one end, and when you close the switch, an electric current
travels through a wire to the other end, where there is a relay that makes
a click. By tapping the switch at one end, you make the relay at the other
end click. Letters and digits are encoded as sequences of short and long
clicks. A short click is called a
<EM>dot</EM>, and a long click is called a <EM>dash</EM>.

<P>
The two most common letters in English text are <CODE>E</CODE> and <CODE>T</CODE>; in Morse code these are represented by a single dot and a single dash,
respectively. The codes for <CODE>I</CODE>, <CODE>A</CODE>, <CODE>N</CODE>, and <CODE>M</CODE>, all common letters, are <CODE>**</CODE>, <CODE>*-</CODE>, <CODE>-*</CODE>, and <CODE>--</CODE>. In contrast, the codes for the uncommon letters <CODE>Q</CODE> and <CODE>Z</CODE> are <CODE>--*-</CODE> and <CODE>--**</CODE>.

<P>
In computer file compression, we do a similar thing. We analyze the
contents of the data, and figure out which symbols are frequent and which
are infrequent. Then we assign short codes to the frequent symbols and long
codes to the infrequent symbols. We write out the coded version of the
file, and that usually makes it smaller.

<P>
<HR>
<H2><A NAME="Ambiguous_codes">Ambiguous codes</A></H2>
<P>
There's a problem with Morse Code: You need a third symbol, typically a
long pause, to separate the dots and dashes that make up one letter from
the dats and dashes that make up the next. Otherwise, if you get
<CODE>*-</CODE>, you don't know whether it's the single letter <CODE>A</CODE> or the two letters <CODE>ET</CODE>---or it might be the first bit of the letter <CODE>R</CODE> or <CODE>L</CODE>. In a long message, all the dots and dashes run together and you get a big
mess that can't be turned back into text. In Morse code, it can be hard to
tell `Eugenia' from `Sofia': Without the interletter pauses, they're both:

<P>
<PRE>                ***---**-****-
</PRE>
<P>
Those interletter spaces take up a lot of transmission time, and it would
be nice if you didn't need them. It turns out that if you arrange the code
properly, you don't. The ambiguity problem with Morse Code occurs because
some codes are <EM>prefixes</EM> of others: There are some letters where the code for the first part of one
letter is just the same as the code for the other letter, but with
something extra tacked on. When you see the shorter code, you don't know if
it's complete or if it should be combined with the following symbols. It
turns out that if no code is a prefix of any other, then the code is
unambiguous.   

<P>
Suppose for simplicity that we only needed to send the letters A, C, E, and
S over the telegraph. Instead of Morse code, we could use the following
code table:

<P>
<PRE>
        A       -
        C       **
        E       *-*
        S       *--
</PRE>
<P>
Suppose we receive the message <CODE>-*****-**--*--*-**--</CODE>. What
was the message? Well, the first symbol is <CODE>-</CODE>, so the
first letter in the message must be <CODE>A</CODE>, because that's the
only letter with a code that starts with a <CODE>-</CODE>. Then the
next two symbols are <CODE>**</CODE>, so the second letter must be a
<CODE>C</CODE>, because all the other codes that start with
<CODE>*</CODE> have a <CODE>-</CODE> after the <CODE>*</CODE> instead
of another <CODE>*</CODE>. Similar reasoning shows that the third
letter is also <CODE>C</CODE>. After that, the code is
<CODE>*-*</CODE>; it must be an <CODE>E</CODE>. We continue through
the message, reading off one letter at a time, and eventually we get
the whole thing this way.

<P>
It's so simple that a computer can decode it, if the computer is equipped
with a decision tree like this one:

<P align=center>
<a name="tree">
<img src="tree0.gif">
</a>
</p>
<P>
Start at <EM>Start</EM>, and look at the symbols in the message one by one. At each stage, follow
the appropriate labelled branch to the next node. If there's a letter at
that node, output the letter and go back to the start node. If there's no
letter at the node, look at the next symbol in the input and continue down
the tree towards the leaves.

<P>
<HR>
<H2><A NAME="Huffman_Coding">Huffman Coding</A></H2>
<P>
Obviously, it's important to choose the right code. If Morse had made
the <CODE>*</CODE> code for <CODE>Z</CODE> and <CODE>**-*</CODE> the
code for <CODE>E</CODE>, he wouldn't be famous.

<P>

Choosing the right code can be tricky. Consider the example of the
previous section, where we only had to code messages that contain
<CODE>A</CODE>, <CODE>C</CODE>, <CODE>E</CODE>, and <CODE>S</CODE>.
The code I showed is good when we expect our messages to contain more
<CODE>A</CODE>'s than <CODE>E</CODE>'s or <CODE>S</CODE>'s. If
<CODE>S</CODE> were very common, we clearly could have done better;
less clearly, if all four letters were about equally common, then we
could still have done better, by assigning each letter a code of the
same length:

<P align=center>
<img src="tree1.gif">
</p>

<P>
Suppose, for example, our message happened to contain 200 of each of the
four letters. Then the first code would use 1800 symbols, and the second
code would use only 1600.

<P>
In 1952, David Huffman discovered a method for producing the
<EM>optimal</EM> unambigous code. For a given set of symbols, if you
know the probability with which each symbol appears in the input, you
can use Huffman's method to construct an unambiguous code that encodes
the typical message with fewer <CODE>*</CODE>'s and <CODE>-</CODE>'s than any other code.

<P>
The method is very simple and ingenious. For concreteness, let's suppose
that the (rather silly) message is 

<P>
<PRE>        THE_THIRSTIEST_SISTERS_TEETH_RESIST_THIS_STRESS
</PRE>
<P>
(I used <CODE>_</CODE> instead of space so that it'll be easier to see.)  

<P>
Start with the table of relative probabilities; you can get this by
counting the number of occurrences of every symbol in the message. This is
called <EM>histogramming</EM>. (A <EM>histogram</EM> is a bar chart;
<EM>histos</EM> is Greek for a beam or a mast.) Here's the histogram for the symbols in our
sample message:

<P align=center>
<table halign=center>
<tr><td>S</td><td>11</td><td><img src=square.gif width=110 height=10></td></tr>
<tr><td>T</td><td>10</td><td><img src=square.gif width=100 height=10></td></tr>
<tr><td>E</td><td> 7</td><td><img src=square.gif width=70 height=10></td></tr>
<tr><td>_</td><td> 6</td><td><img src=square.gif width=60 height=10></td></tr>
<tr><td>I</td><td> 5</td><td><img src=square.gif width=50 height=10></td></tr>
<tr><td>H</td><td> 4</td><td><img src=square.gif width=40 height=10></td></tr>
<tr><td>R</td><td> 4</td><td><img src=square.gif width=40 height=10></td></tr>
</table>
</p>

<P>

Now take the two least common entries in the table, that's
<CODE>H</CODE> and <CODE>R</CODE>. They'll get the longest codes,
because they're least common. We'll simplify this by pretending that
<CODE>H</CODE> and <CODE>R</CODE> are the same, and lumping them
together into one category, which we'll call <CODE>HR</CODE>. Then
we'll assign codes to all the other letters and to
<CODE>HR</CODE>. When we're done, we still have to distinguish between
<CODE>H</CODE> and <CODE>R</CODE>. Now, <CODE>HR</CODE> has some
code. We don't know what it is yet, so let's symbolize it with
<CODE>&lt;HR&gt;</CODE>. We don't really need to use
<CODE>&lt;HR&gt;</CODE> in our message, because the is no such thing
as the letter <CODE>HR</CODE>, so we'll split it in two, and let the
code for <CODE>H</CODE> be <CODE>&lt;HR&gt;*</CODE> and the code for
<CODE>R</CODE> be <CODE>&lt;HR&gt;-</CODE>. As a result of this, the
codes for <CODE>H</CODE> and <CODE>R</CODE> will be longer than the
codes for the other letters, but if that has to appen, it's better for
it to happen for <CODE>H</CODE> and <CODE>R</CODE>, because they are
the least common letters in the message.

<P>
So we will lump <CODE>H</CODE> and <CODE>R</CODE> together and pretend temporarily that they are only
one letter. Our table then looks like this:

<P>
<PRE>
        S       11        R = &lt;HR&gt;-
        T       10        H = &lt;HR&gt;*
        HR       8
        E        7
        _        6
        I        5
</PRE>
<P>
Now we repeat the process. The two least common symbols are
<CODE>I</CODE> and <CODE>_</CODE>. We'll lump them together into a new
`symbol' called <CODE>I_</CODE>, we'll assign finish assigning the
codes to <CODE>S</CODE>, <CODE>T</CODE>, <CODE>HR</CODE>,
<CODE>E</CODE>, and <CODE>I_</CODE>. When we're done, <CODE>I</CODE>
will get the code <CODE>&lt;I_&gt;*</CODE> and <CODE>_</CODE> will get
the code <CODE>&lt;I_&gt;-</CODE>.

<P>
<PRE>
        S       11        R = &lt;HR&gt;-
        I_      11        H = &lt;HR&gt;*
        T       10        _ = &lt;I_&gt;-
        HR       8        I = &lt;I_&gt;*
        E        7
</PRE>
<P>
Then we lump together <CODE>HR</CODE> and <CODE>E</CODE>:

<P>
<PRE>
        HRE     15        R   = &lt;HR&gt;-
        S       11        H   = &lt;HR&gt;*
        I_      11        _   = &lt;I_&gt;- 
        T       10        I   = &lt;I_&gt;*
                          HR  = &lt;HRE&gt;- 
                          E   = &lt;HRE&gt;*
</PRE>
<P>
Then we lump together <CODE>T</CODE> and <CODE>I_</CODE>:

<P>
<PRE>
        I_T     21        R   = &lt;HR&gt;-
        HRE     15        H   = &lt;HR&gt;*
        S       11        _   = &lt;I_&gt;- 
                          I   = &lt;I_&gt;*
                          HR  = &lt;HRE&gt;- 
                          E   = &lt;HRE&gt;*
                          I_  = &lt;I_T&gt;-
                          T   = &lt;I_T&gt;*
</PRE>
<P>
Then we lump together <CODE>S</CODE> and <CODE>HRE</CODE>:

<P>
<PRE>
        SHRE    25        R   = &lt;HR&gt;-
        I_T     21        H   = &lt;HR&gt;*
                          I   = &lt;I_&gt;- 
                          _   = &lt;I_&gt;*
                          HR  = &lt;HRE&gt;- 
                          E   = &lt;HRE&gt;*
                          I_  = &lt;I_T&gt;-
                          T   = &lt;I_T&gt;*
                          S   = &lt;SHRE&gt;-
                          HRE = &lt;SHRE&gt;*
</PRE>
<P>
Now we only have two `symbols' left. There's only one way to assign a
code to two symbols; one of them gets <CODE>*</CODE> and the other
gets <CODE>-</CODE>. It doesn't matter which gets which, so let's say
that <CODE>SHRE</CODE> gets <CODE>*</CODE> and <CODE>I_T</CODE> gets
<CODE>-</CODE>.

<P>
Now the codes fall out of the table we've built up in the right-hand
column: 

<P>
<PRE>
	SHRE = * 
		S = *-
	        HRE = **
	                HR = **-
	                        R = **--
	                        H = **-*
	                E  = ***
	I_T = - 
		I_ = -- 
			_ = --- 
			I = --* 
		T = -*
</PRE>
<P>

<P>
We throw away the codes for the fictitious compound symbols, and we're left
with the real code:

<P>
<PRE>
        S = *-
        T = -*
        E = ***
        _ = ---
        I = --*
        H = **-*
        R = **--
</PRE>
<P>
As promised, the code is unambiguous, because no code is a prefix of any
other code. Our original message encodes like this:

<P>
<PRE>
        -***-****----***-*--***--*--*--*****--*--- 
        *---**--******--*-----*******-***-*---     
        **--****---**--*----***-*--**----          
        *--***--****-*-                            
</PRE>
<P>
For a total of 128 <CODE>*</CODE>'s and <CODE>-</CODE>'s, an average of 2.72 symbols per character,
and a 9.3% improvement over the 141 symbols we would have had to use if we
had given every letter a three-symbol code.

<P>
<HR>
<H2><A NAME="The_Code">The Code</A></H2>
<P>
For this article, I implemented a demonstration module that compresses
files. You can retrieve it from the perl Journal web site at <A
HREF="http://www.tpj.com/">http://www.tpj.com/</A> or from my Perl
Paraphernalia web site at <A
HREF="http://www.plover.com/~mjd/perl/Huffman/">http://www.plover.com/~mjd/perl/Huffman/</A>.
The program <CODE>htest.pl</CODE>
compresses an input and saves it to the file <CODE>/tmp/htest.out</CODE>; then it opens this file, reads in the compressed data, decompresses it,
and prints the result to standard output.  

<P>
Most of the real work is in the <CODE>Huffman</CODE> module that <CODE>htest</CODE> uses. Here are the important functions that <CODE>htest</CODE> calls:

<P>
<PRE>        my $hist = Huffman::histogram(\@symbols);
</PRE>
<P>
This generates the histogram of the input text. The histogram is the tally
of the number of occurrences of each symbol. The argument to
<CODE>histogram</CODE> is an array of symbols, passed by reference for efficiency, because it is
likely to be very large. It might have been simpler to pass the input as a
single string, but this way we can assign codes per-word instead of
per-character if we want to, just by splitting the input into an array in a
different way.

<P>
<PRE>        my $codes = Huffman::tabulate($hist);
</PRE>
<P>
The <CODE>tabulate</CODE> function generates the code table from thie histogram using Huffman's
method. (This has the side effect of destroying the histogram.) The code
table is just a hash that maps symbols to codes; the codes themselves are
strings like <CODE>0010011</CODE>.  

<P>
<PRE>        Huffman::save_code_table(*FILE, $codes);
</PRE>
<P>
This writes the code table to the file.

<P>
<PRE>        Huffman::encode(*FILE, $codes, \@symbols);
</PRE>
<P>
This encodes the input text and writes the result to the file.

<P>
<PRE>
        my $codes = Huffman::load_code_table(*FILE);
        my $coded_data = &lt;FILE&gt;;
        my $text = Huffman::decode($codes, $coded_data);
</PRE>
<P>
This is the decompression process. The return value from
<CODE>load_code_table</CODE> is a code table in a rather interesing form. It's a decision tree, just
like the ones we saw earlier in the article. Each node of the tree is
either a single string (which is the symbol that the decoder should output)
or it's an array with two elements; the 0th element is the part of the tree
on the branch labeled 0, and the 1st element is the part of the tree on the
branch labeled 1. For example, taking <CODE>*</CODE> as equivalent to 0 and <CODE>-</CODE> as
equivalent to 1, the <a href="#tree">decision tree from earlier in the
article</a> would be represented like
this:

<P>
<img src="tree0s.gif" align=right>
<PRE>
        [[C, 
             [E, 
              S
             ]
         ], 
         A
        ]
</PRE>
<br clear=all>
<P>
<HR>
<H2><A NAME="The_Rub">The Rub</A></H2>
<P>
We can compress the file, but unless we include the code table in the
compressed file, we won't be able to uncompress it again. Files that
can't be uncompressed are not very useful. (Sometimes you don't need
to be able to decompress the file; in these cases the Perl
<CODE>unlink</CODE> function implements a much simpler and more
efficient form of compression.)  But sometimes the code table is
bigger than the savings that we got from compressing the file,
especially if the original file is small. On a sample file of
<tt>comp.lang.perl.misc</tt> articles, the compressor reduced the file
size by 32%, from 42733 bytes to 29114. But when I ran it on its own
source code, the file size increased from 987 bytes to 1321. The
compressed data was only 631 bytes long, but the code table and other
overhead took up 690 bytes.

<P>
<HR>
<H2><A NAME="Another_Rub">Another Rub</A></H2>
<P>
Huffman coding is optimal in the sense of compressing a given set of
symbols into the smallest space possible. But if you readjust your idea of
a `symbol', you can get must better compression.

<P>
Suppose you're compressing English text. If you histogram the characters,
and use Huffman's method, you'll get the optimal way to encode the
characters. Because each character has its own code, your code would also
serve to code arbitrary random gibberish. It should be clear that this
expressiveness has a price. For English text, we don't need so much
expressiveness; it's clearly more efficient to assign a code to each <EM>word</EM> instead of to each character. In doing so, we lost the ability to encode
random gibberish, but our compressed data becomes much smaller. Suppose
there are about 2**17 words in English; if we assign each one a different
17-bit code, we can then encode our original seven-word message:

<P>
<PRE>        THE THIRSTIEST SISTERS TEETH RESIST THIS STRESS
</PRE>
<P>
into 7*17 = 119 bits, alreasy an improvement on the 128 we used before. And
if we use Huffman's method to assign long codes to rare words and short
codes to common words, we can do even better.

<P>
Using this method, I compressed the <tt>comp.lang.perl.misc</tt> articles by 63%. Unfortunately, the code table blew up as a result, and
took up 43739 bytes, which was larger than the original uncompressed data.

<P>
<HR>
<H2><A NAME="Other_Methods">Other Methods</A></H2>
<P>
Most modern data compression doesn't use Huffman coding directly. A better
method was proposed in 1977 and 1978 by Jakob Ziv and Abraham Lempel. The
Lempel-Ziv methods scan the input data, and when they find a substring that
occurs twice, the replace the second occurrence with a reference back to
the first. The references can then be Huffman-compressed.  

<P>
These methods have several advantages over the basic scheme I implemented.
One important benefit of Lempel-Ziv compression methods is that they don't
have to read and analyze the entire input in advance; they can construct a
code table as they go along, basing it on only the portion of the input
seen so far. This is important for practical applications. The data
compression in your modem wouldn't be very useful if the modem had to
gather and analyze a week's worth of traffic before it could send or
receive anything.

<p>Another benefit of this method is that because the code table is
imlpicit in the input seen so far, the code table doesn't need to be
recorded explicitly.  The decoding process can infer the code table
from the coded data itself.  This avoids the embarrassing inflamations
that we saw where the code table took up more space than was actually
saved by the compression process.</p>

<P> One extra payoff of building the code table on the fly is that if
the algorithm notices that the code table it's using isn't performing
well, it can throw it away and start over in the middle. If the data
is in several pieces that have very different characters, say a
graphic image embedded in a text file, this method will be a win over
straight Huffman coding because it'll be able to apply an appropriate
code table to each part of the data.  LZW, the compression algorithm
used by the DOS <CODE>lha</CODE> program, doesn't do this, but the
variation of it employed by the Unix <CODE>compress</CODE> program
does. The algorithm used by the GNU project's <CODE>gzip</CODE> and
<CODE>zlib</CODE> is different but also periodically throws away the
code tables and starts over.


<P>
The best thing about Lempel-Ziv and related methods is that they don't need
to decide in advance how to break up the input into symbols. LZW, for
example, puts any string that it hasn't seen before into the code table,
under the assumption that if it appeared once, it'll probably appear again.
As the input comes in, longer and longer substrings of the input go into
the code table; long substrings go in only when their shorter prefixes have
appeared multiple times. This means that if the file is best broken up into
words, the algorithm will eventually detect that; if it is best broken up
into single characters, the algorithm will detect that too.

<hr>
<h2><A NAME="Other_Directions">Other Directions</a></h2>

<p>
        If you want an easy but possibly amusing project, try altering
        the code table functions in the module so that the code tables
        are smaller.  At present, the module  does not assume that
        the coding is done per-character, so you shouldn't break that.
</p>

<p>
        Actually, though, assigning the codes per-word seems to blow
        up the code table more than it saves.  I haven't found a case
        yet where it's worthwhile.  I'd be interested in seeing more
        analysis of this.
</p>

<p>
        Finally, the decision-tree data structure in the decoder is
        probably over-clever.  It would be simpler to represent the
        same information with a <i>state table</i>, this way:
</p>

<pre>
                         0        1
                --------------------
                0        1        A
                1        C        2
                2        E        S
</pre>
        
<p>
        Each internal node in the decision tree is assigned a <i>state</i>,
        which is a number; the root node gets the number 0.  To
        decode, the decoder keeps track of the state number, which
        says what internal node it's currently at.  Each time it reads
        a bit, it looks in the table to decide where to go next.  If
        it sees a symbol, that means it erached a leaf, and should
        output that symbol and start over at state 0; if it sees a
        number, that means it's now at a different internal node and
        should read more bits.  I'm really not sure whether this would
        be more efficient than the way I did do it, but it would
        probably be easier to understand.
</p>

<P>
<HR>
<H2><A NAME="Bibliography">Bibliography</A></H2>
<P>
<EM>Symbols, Signals, and Noise</EM>, J.R. Pierce, pp. 94--97. Harper, New York, 1961.

<P>
The comp.compression FAQ, is an excellent starting source of information,
especially part 2. It is available from <A
HREF="ftp://rtfm.mit.edu/pub/usenet/news.answers/compression-faq/">ftp://rtfm.mit.edu/pub/usenet/news.answers/compression-faq/</A>.


<P>
<HR>
<H2><A NAME="Notes">Notes</A></H2>
<P>
My column finally has a name!  <EM>Bricolage</EM> is a hot word these days in the computer learning business; it's French for <EM>tinkering</EM> or
<EM>pottering</EM>. Thanks to Deven Ullman, who suggested it, and to the other people who
suggested other names.

<hr>

<img src="../pics/small-sigils.gif" align=right>
<p>Return to: 
<a href="http://www.plover.com/~mjd/">Universe of Discourse main page</a> |
<a href="http://www.plover.com/~mjd/whatsnew.html">What's new page</a> |
<a href="../">Perl Paraphernalia</a>|
<a href="./">File Compression Page</a>
</p>

<p><a href="mailto:mjd-tpj-huffman@plover.com">mjd-tpj-huffman@plover.com</a></p>

</BODY>

</HTML>

